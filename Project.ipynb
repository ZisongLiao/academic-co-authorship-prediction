{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train.txt', 'r')\n",
    "train_data = []\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    train_data.append(line)\n",
    "totaledge = 0\n",
    "for edge in train_data:\n",
    "    for node in edge:\n",
    "        totaledge = max(totaledge, int(node))\n",
    "adj_matrix = np.zeros(shape = (totaledge + 1, totaledge + 1))\n",
    "\n",
    "for row in train_data:\n",
    "    for i in range(len(row)):\n",
    "        for j in range(i + 1, len(row)):\n",
    "            adj_matrix[int(row[i])][int(row[j])] += 1\n",
    "            adj_matrix[int(row[j])][int(row[i])] += 1\n",
    "            \n",
    "rows, cols = np.where(adj_matrix > 0)\n",
    "value = adj_matrix[rows, cols]\n",
    "edges = list(zip(rows.tolist(), cols.tolist(), list(adj_matrix[rows, cols].astype(int))))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edges)\n",
    "    \n",
    "neg_rows, neg_cols = np.where(adj_matrix == 0)\n",
    "neg_edges = list(zip(neg_rows.tolist(), neg_cols.tolist(), list(adj_matrix[neg_rows, neg_cols].astype(int))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test-public.csv')\n",
    "test_set = test_set.iloc[:, [1, 2]]\n",
    "test_set = test_set.values.tolist()\n",
    "prediction_shortest = []\n",
    "for i in test_set:\n",
    "    if i[0] in G and i[1] in G and nx.has_path(G, i[0], i[1]):\n",
    "        prediction_shortest.append(nx.shortest_path_length(G, i[0], i[1], weight='weight'))\n",
    "    else:\n",
    "        prediction_shortest.append(9999)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(test_set)):\n",
    "    score = 1 / prediction_shortest[i]\n",
    "    result.append([i + 1, score])\n",
    "columnName = ['Id', 'Predicted']\n",
    "output = pd.DataFrame(columns = columnName, data = result)\n",
    "output.to_csv('output.csv', index = 0)\n",
    "prediction_shortest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## node2vec\n",
    "node2vec = Node2Vec(G, dimensions = 64, walk_length = 10, num_walks = 10, workers = 4)\n",
    "model = node2vec.fit(window = 10, min_count = 1, batch_words = 4)\n",
    "model.wv.most_similar('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('embeddings.txt')\n",
    "emb_file = open('embeddings.txt', 'r')\n",
    "emb_dict = {}\n",
    "emb_file.readline()\n",
    "for line in emb_file:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    for i in range(1, len(line)):\n",
    "        line[i] = float(line[i])\n",
    "    emb_dict[line[0]] = line[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineScore(x, y):\n",
    "    result1 = 0\n",
    "    result2 = 0\n",
    "    result3 = 0\n",
    "    for i in range(len(x)):\n",
    "        result1 += x[i] * y[i]\n",
    "        result2 += x[i] ** 2\n",
    "        result3 += y[i] ** 2\n",
    "    return result1 / ((result2 * result3) ** 0.5)\n",
    "\n",
    "test_set = pd.read_csv('test-public.csv')\n",
    "test_set = test_set.iloc[:, [1, 2]]\n",
    "test_set = test_set.values.tolist()\n",
    "result = []\n",
    "i = 1\n",
    "for pair in test_set:\n",
    "    source = str(pair[0])\n",
    "    sink = str(pair[1])\n",
    "    if source in emb_dict.keys() and sink in emb_dict.keys():\n",
    "        score = cosineScore(emb_dict[source], emb_dict[sink])\n",
    "    else:\n",
    "        score = 0\n",
    "    result.append([i, score])\n",
    "    i += 1\n",
    "columnName = ['Id', 'Predicted']\n",
    "output = pd.DataFrame(columns = columnName, data = result)\n",
    "output.to_csv('output.csv', index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = [], []\n",
    "all_links = edges + neg_edges\n",
    "len(all_links)\n",
    "len(neg_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import multiprocessing\n",
    "from itertools import permutations\n",
    "\n",
    "filepath = 'train.txt'\n",
    "\n",
    "def read_text(filepath):\n",
    "    author_edge = defaultdict(list)\n",
    "    two_way = defaultdict(list)\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for ln in lines:\n",
    "            authors = ln.split()\n",
    "            i = 0\n",
    "            while i < len(authors):\n",
    "                j = i + 1\n",
    "                while j < len(authors):\n",
    "                    if authors[i] != authors[j]:\n",
    "                        if authors[j] not in author_edge[authors[i]]:\n",
    "                            author_edge[authors[i]].append(authors[j])\n",
    "                        if authors[j] not in two_way[authors[i]]:\n",
    "                            two_way[authors[i]].append(authors[j])\n",
    "                        if authors[i] not in two_way[authors[j]]:\n",
    "                            two_way[authors[j]].append(authors[i])\n",
    "                    j += 1\n",
    "                i += 1\n",
    "\n",
    "    return author_edge, two_way\n",
    "\n",
    "def dict_to_list(author_dict):\n",
    "    author_df = pd.DataFrame()\n",
    "    author_1 = []\n",
    "    author_2 = []\n",
    "\n",
    "    for key, item in author_dict.items():\n",
    "        for i in item:\n",
    "            author_1.append(int(key))\n",
    "            author_2.append(int(i))\n",
    "\n",
    "    author_df['a1'] = author_1\n",
    "    author_df['a2'] = author_2\n",
    "\n",
    "    return author_df, author_1, author_2\n",
    "\n",
    "def weighted_edge(filepath):\n",
    "    a1 = []\n",
    "    a2 = []\n",
    "    edge = pd.DataFrame()\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for ln in lines:\n",
    "            authors = ln.split()\n",
    "            for i in range(len(authors)):\n",
    "                for j in range(i+1, len(authors)):\n",
    "                    a1.append(authors[i])\n",
    "                    a2.append(authors[j])\n",
    "    edge['a1'] = a1\n",
    "    edge['a2'] = a2\n",
    "\n",
    "    return edge\n",
    "\n",
    "wedge = weighted_edge(filepath)\n",
    "print(wedge.shape)\n",
    "author_dict,_ = read_text(filepath)\n",
    "author_df, author1, author2 = dict_to_list(author_dict)\n",
    "author_df['link'] = 1\n",
    "print(author_df.shape)\n",
    "author_list = author1 + author2\n",
    "author_list = list(dict.fromkeys(author_list))\n",
    "author_list.sort()\n",
    "print(len(author_list))\n",
    "print(author_list)\n",
    "print(author_list[0], author_list[-1])\n",
    "missing_node = list(set(range(author_list[-1]+1))-set(author_list))\n",
    "print(len(missing_node))\n",
    "author_graph = nx.convert_matrix.from_pandas_edgelist(author_df, 'a1', 'a2')\n",
    "adj_matrix = nx.to_numpy_matrix(author_graph, nodelist=author_list)\n",
    "print(adj_matrix.shape)\n",
    "\n",
    "def get_unconnected_author(adj_matrix, author_list):\n",
    "    unconnected = []\n",
    "    i = 0\n",
    "    while i < adj_matrix.shape[0]:\n",
    "        j = 0\n",
    "        while j < i:\n",
    "            if adj_matrix.item((i, j)) == 0:\n",
    "                unconnected.append((author_list[i], author_list[j]))\n",
    "\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return unconnected\n",
    "\n",
    "unconnected_list = get_unconnected_author(adj_matrix, author_list)\n",
    "print(len(unconnected_list))\n",
    "a1_uc = [i[0] for i in unconnected_list]\n",
    "a2_uc = [i[1] for i in unconnected_list]\n",
    "uc_df = pd.DataFrame({'a1': a1_uc, 'a2': a2_uc})\n",
    "uc_df['link'] = 0\n",
    "author_graph.add_nodes_from(missing_node)\n",
    "_,uc_select = train_test_split(uc_df, test_size=author_df.shape[0])\n",
    "print(uc_select)\n",
    "data = pd.concat([author_df, uc_select])\n",
    "print(data)\n",
    "author_graph = nx.convert_matrix.from_pandas_edgelist(wedge, 'a1', 'a2')\n",
    "author_graph.add_nodes_from(missing_node)\n",
    "n2v = Node2Vec(author_graph, p=10, q=0.1, workers=multiprocessing.cpu_count()-1)\n",
    "n2v_model = n2v.fit(window=10, min_count=1)\n",
    "x = [(n2v_model.wv[str(i)]+n2v_model.wv[str(j)]) for i, j in zip(data['a1'], data['a2'])]\n",
    "y = data['link'].to_list()\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(np.array(x), data['link'], test_size=0.3, stratify=data['link'])\n",
    "mlp = MLPClassifier(hidden_layer_sizes=100, activation='relu', solver='adam')\n",
    "mlp.fit(xtrain, ytrain)\n",
    "predict_proba = mlp.predict_proba(xtest)[:,1]\n",
    "print('Test set AUC: ', roc_auc_score(ytest, predict_proba))\n",
    "test_file = 'test-public.csv'\n",
    "test = pd.read_csv(test_file)\n",
    "t_x = [(n2v_model.wv[str(i)]+n2v_model.wv[str(j)]) for i,j in zip(test['Source'], test['Sink'])]\n",
    "t_predictions = mlp.predict_proba(t_x)\n",
    "print(t_predictions[:,1])\n",
    "pred = t_predictions[:,1]\n",
    "index = list(range(1, len(pred)+1))\n",
    "pred_data = pd.DataFrame()\n",
    "pred_data['id'] = index\n",
    "pred_data['Predicted'] = pred\n",
    "pred_data.to_csv('mlp.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
